{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883e740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cem_optimizer_v2 import CEM_opt\n",
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from buffers import MultiEnvReplayBuffer\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from queue import Queue\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6d9e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\n",
    "\n",
    "def json_load():\n",
    "    with open('config_parameters.json', 'r') as f:\n",
    "        config_file = json.load(f)\n",
    "    return config_file\n",
    "\n",
    "\n",
    "def save_json_update(json_obj):\n",
    "    with open('config_parameters.json', 'w') as f:\n",
    "        json.dump(json_obj, f)\n",
    "\n",
    "config = json_load()\n",
    "\n",
    "\n",
    "# nested struct\n",
    "buff_config = config['buffer']\n",
    "planner_config = config['planner']\n",
    "const_config = config['const']\n",
    "train_config = config['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f889de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dema/PycharmProjects/lifelong_rl/venv/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/dema/PycharmProjects/lifelong_rl/venv/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "envs = {}\n",
    "\n",
    "for name in list(ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE.keys()):\n",
    "    if not buff_config[\"correspondence_id2env\"].get(name, 0):\n",
    "        buff_config[\"correspondence_id2env\"][name] = buff_config[\"correspondence_id2env\"][\"first_idx_free\"]\n",
    "        buff_config[\"correspondence_id2env\"][\"first_idx_free\"] += 1\n",
    "    envs[buff_config[\"correspondence_id2env\"][name]] = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[name]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f176dc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PlateSlideSideV2GoalObservable instance>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "201b1569",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = MultiEnvReplayBuffer(1_000)\n",
    "env_action_space_shape = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b15602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformedPlanner:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env\n",
    "        self.horizon = 40\n",
    "        self.num_sequence_action = 100\n",
    "        self.cem = CEM_opt(num_action_seq=self.num_sequence_action,\n",
    "                           action_seq_len=env_action_space_shape * self.horizon,\n",
    "                           percent_elite=0.1)\n",
    "        self.action_seq_planned = Queue(maxsize=self.horizon)\n",
    "        \n",
    "        \n",
    "    def plan(self, force_replan=False):\n",
    "        \n",
    "        if self.action_seq_planned.empty() or force_replan:\n",
    "            \n",
    "            action_sequences = self.cem.population\n",
    "            rewards = np.zeros(action_sequences.shape[0])\n",
    "            for idx, seq in enumerate(action_sequences):\n",
    "                rewards[idx] = self.eval_act_seq(seq)\n",
    "            self.cem.update(rewards)\n",
    "            \n",
    "            for act in self.cem.solutions().reshape(-1, 4):\n",
    "                self.action_seq_planned.put(act)\n",
    "        return self.action_seq_planned.get()\n",
    "            \n",
    "            \n",
    "                    \n",
    "    def eval_act_seq(self, sequence):\n",
    "        rew_seq = 0\n",
    "        self.env.reset()\n",
    "        act_reshaped = sequence.reshape((-1, 4))\n",
    "        for act in act_reshaped:\n",
    "            \n",
    "            _, r, _, _ = self.env.step(act)\n",
    "            rew_seq += r\n",
    "        return rew_seq/len(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa6ad1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_k=22, episode=19, horizon_step=0, reward=0.9360255775307187, done=False\n",
      "env_k=22, episode=19, horizon_step=1, reward=0.9377559178867427, done=False\n",
      "env_k=22, episode=19, horizon_step=2, reward=0.940641457766598, done=False\n",
      "env_k=22, episode=19, horizon_step=3, reward=0.9444797164955022, done=False\n",
      "env_k=22, episode=19, horizon_step=4, reward=0.9491589289275113, done=False\n",
      "env_k=22, episode=19, horizon_step=5, reward=0.9543104360373054, done=False\n",
      "env_k=22, episode=19, horizon_step=6, reward=0.9597786329688927, done=False\n",
      "env_k=22, episode=19, horizon_step=7, reward=0.9652745176253463, done=False\n",
      "env_k=22, episode=19, horizon_step=8, reward=0.9706125893449556, done=False\n",
      "env_k=22, episode=19, horizon_step=9, reward=0.9756192665127338, done=False\n",
      "env_k=22, episode=19, horizon_step=10, reward=0.9800919051040808, done=False\n",
      "env_k=22, episode=19, horizon_step=11, reward=0.9839795160886217, done=False\n",
      "env_k=22, episode=19, horizon_step=12, reward=0.9872949167115987, done=False\n",
      "env_k=22, episode=19, horizon_step=13, reward=0.9900443402953281, done=False\n",
      "env_k=22, episode=19, horizon_step=14, reward=0.9923065585062262, done=False\n",
      "env_k=22, episode=19, horizon_step=15, reward=0.9941270898422232, done=False\n",
      "env_k=22, episode=19, horizon_step=16, reward=0.9955198910802388, done=False\n",
      "env_k=22, episode=19, horizon_step=17, reward=0.9782392287410844, done=False\n",
      "env_k=22, episode=19, horizon_step=18, reward=0.9748391260702162, done=False\n",
      "env_k=22, episode=19, horizon_step=19, reward=0.9774422081383625, done=False\n",
      "env_k=22, episode=19, horizon_step=20, reward=0.9795115752473913, done=False\n",
      "env_k=22, episode=19, horizon_step=21, reward=0.9809820785218895, done=False\n",
      "env_k=22, episode=19, horizon_step=22, reward=0.9820186980806909, done=False\n",
      "env_k=22, episode=19, horizon_step=23, reward=0.9827567991157833, done=False\n",
      "env_k=22, episode=19, horizon_step=24, reward=0.9832921187727982, done=False\n",
      "env_k=22, episode=19, horizon_step=25, reward=0.9836819961267412, done=False\n",
      "env_k=22, episode=19, horizon_step=26, reward=0.9839475535781057, done=False\n",
      "env_k=22, episode=19, horizon_step=27, reward=0.9841324982097176, done=False\n",
      "env_k=22, episode=19, horizon_step=28, reward=0.98426103959261, done=False\n",
      "env_k=22, episode=19, horizon_step=29, reward=0.9843412333541284, done=False\n",
      "env_k=22, episode=19, horizon_step=30, reward=0.9843816824792638, done=False\n",
      "env_k=22, episode=19, horizon_step=31, reward=0.9844203053402599, done=False\n",
      "env_k=22, episode=19, horizon_step=32, reward=0.984436489160515, done=False\n",
      "env_k=22, episode=19, horizon_step=33, reward=0.9844375485239145, done=False\n",
      "env_k=22, episode=19, horizon_step=34, reward=0.9844131120882043, done=False\n",
      "env_k=22, episode=19, horizon_step=35, reward=0.9843602479347652, done=False\n",
      "env_k=22, episode=19, horizon_step=36, reward=0.9842856133056176, done=False\n",
      "env_k=22, episode=19, horizon_step=37, reward=0.9841981524957397, done=False\n",
      "env_k=22, episode=19, horizon_step=38, reward=0.9841068566240638, done=False\n",
      "env_k=22, episode=19, horizon_step=39, reward=0.9840224698372847, done=False\n"
     ]
    }
   ],
   "source": [
    "planners = {k: InformedPlanner(envs[k]) for  k in list(envs.keys())}\n",
    "\n",
    "\n",
    "for env_k in list(planners.keys()): \n",
    "    \n",
    "    min_rew = 100\n",
    "    max_rew = 0\n",
    "    \n",
    "    for episode in range(20): \n",
    "        \n",
    "        \n",
    "        state = envs[env_k].reset()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for horizon_step in range(40): \n",
    "\n",
    "\n",
    "            action = planners[env_k].plan()\n",
    "            s_prime, reward, done, _ = envs[env_k].step(action)\n",
    "            \n",
    "            buffer.add(state, action, reward, s_prime, done, env_k)\n",
    "            state = s_prime\n",
    "            if reward > max_rew:\n",
    "                max_rew = reward\n",
    "            \n",
    "            if reward < min_rew: \n",
    "                min_rew = reward\n",
    "\n",
    "            print(f'{env_k=}, {episode=}, {horizon_step=}, {reward=}, {done=}')\n",
    "\n",
    "            \n",
    "    with open('log_cem_fake.txt', 'a+') as f: \n",
    "        f.write(f'env {env_k} : envs : {envs[env_k].__str__()} : {min_rew=}, {max_rew=} (horizon=40)\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e7b0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.write_buffer('buffer_stock/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VBLRL",
   "language": "python",
   "name": "vblrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
